{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "n_test_batches = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import syft as sy\n",
    "hook = sy.TorchHook(torch) \n",
    "client = sy.VirtualWorker(hook, id=\"client\")\n",
    "bob = sy.VirtualWorker(hook, id=\"bob\")\n",
    "alice = sy.VirtualWorker(hook, id=\"alice\")\n",
    "crypto_provider = sy.VirtualWorker(hook, id=\"crypto_provider\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arguments():\n",
    "    def __init__(self):\n",
    "        self.batch_size = 64\n",
    "        self.test_batch_size = 50\n",
    "        self.epochs = epochs\n",
    "        self.lr = 0.001\n",
    "        self.log_interval = 100\n",
    "\n",
    "args = Arguments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=args.batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=args.test_batch_size, shuffle=True)\n",
    "\n",
    "private_test_loader = []\n",
    "for data, target in test_loader:\n",
    "    private_test_loader.append((\n",
    "        data.fix_precision().share(alice, bob, crypto_provider=crypto_provider, protocol=\"fss\"),\n",
    "        target.fix_precision().share(alice, bob, crypto_provider=crypto_provider, protocol=\"fss\")\n",
    "    ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, model, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        output = F.log_softmax(output, dim=1)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * args.batch_size, len(train_loader) * args.batch_size,\n",
    "                100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60032 (0%)]\tLoss: 2.280699\n",
      "Train Epoch: 1 [6400/60032 (11%)]\tLoss: 0.302638\n",
      "Train Epoch: 1 [12800/60032 (21%)]\tLoss: 0.357552\n",
      "Train Epoch: 1 [19200/60032 (32%)]\tLoss: 0.162103\n",
      "Train Epoch: 1 [25600/60032 (43%)]\tLoss: 0.116576\n",
      "Train Epoch: 1 [32000/60032 (53%)]\tLoss: 0.211301\n",
      "Train Epoch: 1 [38400/60032 (64%)]\tLoss: 0.115454\n",
      "Train Epoch: 1 [44800/60032 (75%)]\tLoss: 0.051757\n",
      "Train Epoch: 1 [51200/60032 (85%)]\tLoss: 0.034267\n",
      "Train Epoch: 1 [57600/60032 (96%)]\tLoss: 0.286765\n",
      "Train Epoch: 2 [0/60032 (0%)]\tLoss: 0.023893\n",
      "Train Epoch: 2 [6400/60032 (11%)]\tLoss: 0.062522\n",
      "Train Epoch: 2 [12800/60032 (21%)]\tLoss: 0.062613\n",
      "Train Epoch: 2 [19200/60032 (32%)]\tLoss: 0.090940\n",
      "Train Epoch: 2 [25600/60032 (43%)]\tLoss: 0.062988\n",
      "Train Epoch: 2 [32000/60032 (53%)]\tLoss: 0.070532\n",
      "Train Epoch: 2 [38400/60032 (64%)]\tLoss: 0.016616\n",
      "Train Epoch: 2 [44800/60032 (75%)]\tLoss: 0.037788\n",
      "Train Epoch: 2 [51200/60032 (85%)]\tLoss: 0.004919\n",
      "Train Epoch: 2 [57600/60032 (96%)]\tLoss: 0.011107\n",
      "Train Epoch: 3 [0/60032 (0%)]\tLoss: 0.027349\n",
      "Train Epoch: 3 [6400/60032 (11%)]\tLoss: 0.038879\n",
      "Train Epoch: 3 [12800/60032 (21%)]\tLoss: 0.141375\n",
      "Train Epoch: 3 [19200/60032 (32%)]\tLoss: 0.006672\n",
      "Train Epoch: 3 [25600/60032 (43%)]\tLoss: 0.042533\n",
      "Train Epoch: 3 [32000/60032 (53%)]\tLoss: 0.041401\n",
      "Train Epoch: 3 [38400/60032 (64%)]\tLoss: 0.057727\n",
      "Train Epoch: 3 [44800/60032 (75%)]\tLoss: 0.008275\n",
      "Train Epoch: 3 [51200/60032 (85%)]\tLoss: 0.068062\n",
      "Train Epoch: 3 [57600/60032 (96%)]\tLoss: 0.016974\n",
      "Train Epoch: 4 [0/60032 (0%)]\tLoss: 0.022596\n",
      "Train Epoch: 4 [6400/60032 (11%)]\tLoss: 0.030501\n",
      "Train Epoch: 4 [12800/60032 (21%)]\tLoss: 0.008332\n",
      "Train Epoch: 4 [19200/60032 (32%)]\tLoss: 0.011691\n",
      "Train Epoch: 4 [25600/60032 (43%)]\tLoss: 0.010060\n",
      "Train Epoch: 4 [32000/60032 (53%)]\tLoss: 0.053996\n",
      "Train Epoch: 4 [38400/60032 (64%)]\tLoss: 0.003767\n",
      "Train Epoch: 4 [44800/60032 (75%)]\tLoss: 0.112814\n",
      "Train Epoch: 4 [51200/60032 (85%)]\tLoss: 0.031283\n",
      "Train Epoch: 4 [57600/60032 (96%)]\tLoss: 0.007662\n",
      "Train Epoch: 5 [0/60032 (0%)]\tLoss: 0.008479\n",
      "Train Epoch: 5 [6400/60032 (11%)]\tLoss: 0.005484\n",
      "Train Epoch: 5 [12800/60032 (21%)]\tLoss: 0.012783\n",
      "Train Epoch: 5 [19200/60032 (32%)]\tLoss: 0.034548\n",
      "Train Epoch: 5 [25600/60032 (43%)]\tLoss: 0.014103\n",
      "Train Epoch: 5 [32000/60032 (53%)]\tLoss: 0.009231\n",
      "Train Epoch: 5 [38400/60032 (64%)]\tLoss: 0.040691\n",
      "Train Epoch: 5 [44800/60032 (75%)]\tLoss: 0.034995\n",
      "Train Epoch: 5 [51200/60032 (85%)]\tLoss: 0.107417\n",
      "Train Epoch: 5 [57600/60032 (96%)]\tLoss: 0.003075\n",
      "Train Epoch: 6 [0/60032 (0%)]\tLoss: 0.013340\n",
      "Train Epoch: 6 [6400/60032 (11%)]\tLoss: 0.012997\n",
      "Train Epoch: 6 [12800/60032 (21%)]\tLoss: 0.006737\n",
      "Train Epoch: 6 [19200/60032 (32%)]\tLoss: 0.025948\n",
      "Train Epoch: 6 [25600/60032 (43%)]\tLoss: 0.004261\n",
      "Train Epoch: 6 [32000/60032 (53%)]\tLoss: 0.018167\n",
      "Train Epoch: 6 [38400/60032 (64%)]\tLoss: 0.011776\n",
      "Train Epoch: 6 [44800/60032 (75%)]\tLoss: 0.069752\n",
      "Train Epoch: 6 [51200/60032 (85%)]\tLoss: 0.125110\n",
      "Train Epoch: 6 [57600/60032 (96%)]\tLoss: 0.007581\n",
      "Train Epoch: 7 [0/60032 (0%)]\tLoss: 0.002336\n",
      "Train Epoch: 7 [6400/60032 (11%)]\tLoss: 0.002033\n",
      "Train Epoch: 7 [12800/60032 (21%)]\tLoss: 0.008234\n",
      "Train Epoch: 7 [19200/60032 (32%)]\tLoss: 0.009310\n",
      "Train Epoch: 7 [25600/60032 (43%)]\tLoss: 0.017870\n",
      "Train Epoch: 7 [32000/60032 (53%)]\tLoss: 0.004377\n",
      "Train Epoch: 7 [38400/60032 (64%)]\tLoss: 0.006521\n",
      "Train Epoch: 7 [44800/60032 (75%)]\tLoss: 0.009495\n",
      "Train Epoch: 7 [51200/60032 (85%)]\tLoss: 0.004039\n",
      "Train Epoch: 7 [57600/60032 (96%)]\tLoss: 0.095638\n",
      "Train Epoch: 8 [0/60032 (0%)]\tLoss: 0.000354\n",
      "Train Epoch: 8 [6400/60032 (11%)]\tLoss: 0.003240\n",
      "Train Epoch: 8 [12800/60032 (21%)]\tLoss: 0.003775\n",
      "Train Epoch: 8 [19200/60032 (32%)]\tLoss: 0.055964\n",
      "Train Epoch: 8 [25600/60032 (43%)]\tLoss: 0.008151\n",
      "Train Epoch: 8 [32000/60032 (53%)]\tLoss: 0.000450\n",
      "Train Epoch: 8 [38400/60032 (64%)]\tLoss: 0.003285\n",
      "Train Epoch: 8 [44800/60032 (75%)]\tLoss: 0.037890\n",
      "Train Epoch: 8 [51200/60032 (85%)]\tLoss: 0.068297\n",
      "Train Epoch: 8 [57600/60032 (96%)]\tLoss: 0.019847\n",
      "Train Epoch: 9 [0/60032 (0%)]\tLoss: 0.050916\n",
      "Train Epoch: 9 [6400/60032 (11%)]\tLoss: 0.003868\n",
      "Train Epoch: 9 [12800/60032 (21%)]\tLoss: 0.023517\n",
      "Train Epoch: 9 [19200/60032 (32%)]\tLoss: 0.000337\n",
      "Train Epoch: 9 [25600/60032 (43%)]\tLoss: 0.061185\n",
      "Train Epoch: 9 [32000/60032 (53%)]\tLoss: 0.044272\n",
      "Train Epoch: 9 [38400/60032 (64%)]\tLoss: 0.011764\n",
      "Train Epoch: 9 [44800/60032 (75%)]\tLoss: 0.000635\n",
      "Train Epoch: 9 [51200/60032 (85%)]\tLoss: 0.001594\n",
      "Train Epoch: 9 [57600/60032 (96%)]\tLoss: 0.003956\n",
      "Train Epoch: 10 [0/60032 (0%)]\tLoss: 0.031221\n",
      "Train Epoch: 10 [6400/60032 (11%)]\tLoss: 0.013538\n",
      "Train Epoch: 10 [12800/60032 (21%)]\tLoss: 0.016056\n",
      "Train Epoch: 10 [19200/60032 (32%)]\tLoss: 0.006195\n",
      "Train Epoch: 10 [25600/60032 (43%)]\tLoss: 0.007494\n",
      "Train Epoch: 10 [32000/60032 (53%)]\tLoss: 0.137760\n",
      "Train Epoch: 10 [38400/60032 (64%)]\tLoss: 0.008819\n",
      "Train Epoch: 10 [44800/60032 (75%)]\tLoss: 0.007422\n",
      "Train Epoch: 10 [51200/60032 (85%)]\tLoss: 0.020478\n",
      "Train Epoch: 10 [57600/60032 (96%)]\tLoss: 0.000218\n"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    train(args, model, train_loader, optimizer, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(args, model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = model(data)\n",
    "            output = F.log_softmax(output, dim=1)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            pred = output.argmax(1, keepdim=True) # get the index of the max log-probability \n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.1027, Accuracy: 9753/10000 (98%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(args, model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (fc1): Linear(in_features=784, out_features=500, bias=True)\n",
       "  (fc2): Linear(in_features=500, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fix_precision().share(alice, bob, crypto_provider=crypto_provider, protocol=\"fss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(args, model, test_loader):\n",
    "    model.eval()\n",
    "    n_correct_priv = 0\n",
    "    n_total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader[:n_test_batches]:\n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1) \n",
    "            n_correct_priv += pred.eq(target.view_as(pred)).sum()\n",
    "            n_total += args.test_batch_size\n",
    "\n",
    "            n_correct = n_correct_priv.copy().get().float_precision().long().item()\n",
    "    \n",
    "            print('Test set: Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "                n_correct, n_total,\n",
    "                100. * n_correct / n_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Accuracy: -9476747264/50 (-18953494528%)\n",
      "Test set: Accuracy: -25386237952/100 (-25386237952%)\n",
      "Test set: Accuracy: -19945598976/150 (-13297065984%)\n",
      "Test set: Accuracy: -19945598976/200 (-9972799488%)\n",
      "Test set: Accuracy: -9771836416/250 (-3908734566%)\n",
      "Test set: Accuracy: -52721508352/300 (-17573836117%)\n",
      "Test set: Accuracy: -52721508352/350 (-15063288101%)\n",
      "Test set: Accuracy: -52127395840/400 (-13031848960%)\n",
      "Test set: Accuracy: -51503710208/450 (-11445268935%)\n",
      "Test set: Accuracy: -51503710208/500 (-10300742042%)\n",
      "Test set: Accuracy: -51503710208/550 (-9364310947%)\n",
      "Test set: Accuracy: -58668490752/600 (-9778081792%)\n",
      "Test set: Accuracy: -58668490752/650 (-9025921654%)\n",
      "Test set: Accuracy: -58668490752/700 (-8381212965%)\n",
      "Test set: Accuracy: -58668490752/750 (-7822465434%)\n",
      "Test set: Accuracy: -54498357248/800 (-6812294656%)\n",
      "Test set: Accuracy: -54498357248/850 (-6411571441%)\n",
      "Test set: Accuracy: -54498357248/900 (-6055373028%)\n",
      "Test set: Accuracy: -56317009920/950 (-5928106307%)\n",
      "Test set: Accuracy: -56317009920/1000 (-5631700992%)\n",
      "Test set: Accuracy: -56765947904/1050 (-5406280753%)\n",
      "Test set: Accuracy: -60832755712/1100 (-5530250519%)\n",
      "Test set: Accuracy: -60052844544/1150 (-5221986482%)\n",
      "Test set: Accuracy: -60052844544/1200 (-5004403712%)\n",
      "Test set: Accuracy: -60052844544/1250 (-4804227564%)\n",
      "Test set: Accuracy: -60052844544/1300 (-4619449580%)\n",
      "Test set: Accuracy: -60052844544/1350 (-4448358855%)\n",
      "Test set: Accuracy: -58565496832/1400 (-4183249774%)\n",
      "Test set: Accuracy: -62984990720/1450 (-4343792463%)\n",
      "Test set: Accuracy: -62984990720/1500 (-4198999381%)\n",
      "Test set: Accuracy: -58471280640/1550 (-3772340686%)\n",
      "Test set: Accuracy: -60428537856/1600 (-3776783616%)\n",
      "Test set: Accuracy: -63947407360/1650 (-3875600446%)\n",
      "Test set: Accuracy: -45160079360/1700 (-2656475256%)\n",
      "Test set: Accuracy: -46182023168/1750 (-2638972752%)\n",
      "Test set: Accuracy: -46182023168/1800 (-2565667954%)\n",
      "Test set: Accuracy: -45709983744/1850 (-2470809932%)\n",
      "Test set: Accuracy: -45709983744/1900 (-2405788618%)\n",
      "Test set: Accuracy: -46987915264/1950 (-2409636680%)\n",
      "Test set: Accuracy: -65266597888/2000 (-3263329894%)\n",
      "Test set: Accuracy: -65266597888/2050 (-3183736482%)\n",
      "Test set: Accuracy: -66542596096/2100 (-3168695052%)\n",
      "Test set: Accuracy: -63603294208/2150 (-2958292754%)\n",
      "Test set: Accuracy: -58794176512/2200 (-2672462569%)\n",
      "Test set: Accuracy: -58659565568/2250 (-2607091803%)\n",
      "Test set: Accuracy: -42157875200/2300 (-1832951096%)\n",
      "Test set: Accuracy: -41939177472/2350 (-1784645850%)\n",
      "Test set: Accuracy: -41203703808/2400 (-1716820992%)\n",
      "Test set: Accuracy: -41203703808/2450 (-1681783829%)\n",
      "Test set: Accuracy: -43996426240/2500 (-1759857050%)\n",
      "Test set: Accuracy: -44283277312/2550 (-1736599110%)\n",
      "Test set: Accuracy: -44524056576/2600 (-1712463714%)\n",
      "Test set: Accuracy: -35540017152/2650 (-1341132723%)\n",
      "Test set: Accuracy: -36082659328/2700 (-1336394790%)\n",
      "Test set: Accuracy: -35347021824/2750 (-1285346248%)\n",
      "Test set: Accuracy: -65943621632/2800 (-2355129344%)\n",
      "Test set: Accuracy: -69648031744/2850 (-2443790588%)\n",
      "Test set: Accuracy: -69054210048/2900 (-2381179657%)\n",
      "Test set: Accuracy: -73251315712/2950 (-2483095448%)\n",
      "Test set: Accuracy: -82337841152/3000 (-2744594705%)\n",
      "Test set: Accuracy: -82337841152/3050 (-2699601349%)\n",
      "Test set: Accuracy: -79277178880/3100 (-2557328351%)\n",
      "Test set: Accuracy: -77480681472/3150 (-2459704174%)\n",
      "Test set: Accuracy: -73713500160/3200 (-2303546880%)\n",
      "Test set: Accuracy: -73713500160/3250 (-2268107697%)\n",
      "Test set: Accuracy: -74562560000/3300 (-2259471515%)\n",
      "Test set: Accuracy: -77780738048/3350 (-2321813076%)\n",
      "Test set: Accuracy: -76201500672/3400 (-2241220608%)\n",
      "Test set: Accuracy: -66452971520/3450 (-1926173088%)\n",
      "Test set: Accuracy: -66452967424/3500 (-1898656212%)\n",
      "Test set: Accuracy: -66452967424/3550 (-1871914575%)\n",
      "Test set: Accuracy: -67277676544/3600 (-1868824348%)\n",
      "Test set: Accuracy: -67277676544/3650 (-1843224015%)\n",
      "Test set: Accuracy: -69404983296/3700 (-1875810359%)\n",
      "Test set: Accuracy: -69404983296/3750 (-1850799555%)\n",
      "Test set: Accuracy: -76753149952/3800 (-2019819736%)\n",
      "Test set: Accuracy: -76753149952/3850 (-1993588310%)\n",
      "Test set: Accuracy: -76753149952/3900 (-1968029486%)\n",
      "Test set: Accuracy: -77460553728/3950 (-1961026677%)\n",
      "Test set: Accuracy: -77460553728/4000 (-1936513843%)\n",
      "Test set: Accuracy: -74789396480/4050 (-1846651765%)\n",
      "Test set: Accuracy: -74596564992/4100 (-1819428414%)\n",
      "Test set: Accuracy: -74596564992/4150 (-1797507590%)\n",
      "Test set: Accuracy: -62131343360/4200 (-1479317699%)\n",
      "Test set: Accuracy: -49411371008/4250 (-1162620494%)\n",
      "Test set: Accuracy: -35373535232/4300 (-822640354%)\n",
      "Test set: Accuracy: -35373535232/4350 (-813184718%)\n",
      "Test set: Accuracy: -35373535232/4400 (-803943983%)\n",
      "Test set: Accuracy: -35373535232/4450 (-794910904%)\n",
      "Test set: Accuracy: -35373535232/4500 (-786078561%)\n",
      "Test set: Accuracy: -34677293056/4550 (-762138309%)\n",
      "Test set: Accuracy: -34677293056/4600 (-753854197%)\n",
      "Test set: Accuracy: -34677293056/4650 (-745748238%)\n",
      "Test set: Accuracy: -31109318656/4700 (-661900397%)\n",
      "Test set: Accuracy: -31692763136/4750 (-667216066%)\n",
      "Test set: Accuracy: -31692763136/4800 (-660265899%)\n",
      "Test set: Accuracy: -31692763136/4850 (-653459034%)\n",
      "Test set: Accuracy: -31692763136/4900 (-646791084%)\n",
      "Test set: Accuracy: -31692763136/4950 (-640257841%)\n",
      "Test set: Accuracy: -32639062016/5000 (-652781240%)\n",
      "Test set: Accuracy: -32639059968/5050 (-646318019%)\n",
      "Test set: Accuracy: -30188621824/5100 (-591933761%)\n",
      "Test set: Accuracy: -31381929984/5150 (-609357864%)\n",
      "Test set: Accuracy: -34333106176/5200 (-660252042%)\n",
      "Test set: Accuracy: -34333106176/5250 (-653963927%)\n",
      "Test set: Accuracy: -35426832384/5300 (-668430800%)\n",
      "Test set: Accuracy: -37369147392/5350 (-698488736%)\n",
      "Test set: Accuracy: -44981166080/5400 (-832984557%)\n",
      "Test set: Accuracy: -44981166080/5450 (-825342497%)\n",
      "Test set: Accuracy: -50233221120/5500 (-913331293%)\n",
      "Test set: Accuracy: -50233221120/5550 (-905103083%)\n",
      "Test set: Accuracy: -50233221120/5600 (-897021806%)\n",
      "Test set: Accuracy: -50233221120/5650 (-889083560%)\n",
      "Test set: Accuracy: -76007112704/5700 (-1333458118%)\n",
      "Test set: Accuracy: -78935670784/5750 (-1372794275%)\n",
      "Test set: Accuracy: -78573600768/5800 (-1354717255%)\n",
      "Test set: Accuracy: -78573600768/5850 (-1343138475%)\n",
      "Test set: Accuracy: -74445938688/5900 (-1261795571%)\n",
      "Test set: Accuracy: -77586694144/5950 (-1303978053%)\n",
      "Test set: Accuracy: -80302129152/6000 (-1338368819%)\n",
      "Test set: Accuracy: -77179207680/6050 (-1275689383%)\n",
      "Test set: Accuracy: -77179207680/6100 (-1265232913%)\n",
      "Test set: Accuracy: -78267047936/6150 (-1272634926%)\n",
      "Test set: Accuracy: -80874471424/6200 (-1304426958%)\n",
      "Test set: Accuracy: -53184450560/6250 (-850951209%)\n",
      "Test set: Accuracy: -53184450560/6300 (-844197628%)\n",
      "Test set: Accuracy: -30068889600/6350 (-473525820%)\n",
      "Test set: Accuracy: -39827734528/6400 (-622308352%)\n",
      "Test set: Accuracy: -39072669696/6450 (-605777825%)\n",
      "Test set: Accuracy: -37405896704/6500 (-575475334%)\n",
      "Test set: Accuracy: -40558346240/6550 (-619211393%)\n",
      "Test set: Accuracy: -40558346240/6600 (-614520398%)\n",
      "Test set: Accuracy: -62101848064/6650 (-933862377%)\n",
      "Test set: Accuracy: -61959839744/6700 (-924773728%)\n",
      "Test set: Accuracy: -62429974528/6750 (-924888512%)\n",
      "Test set: Accuracy: -62429974528/6800 (-918087861%)\n",
      "Test set: Accuracy: -59622723584/6850 (-870404724%)\n",
      "Test set: Accuracy: -53764472832/6900 (-779195258%)\n",
      "Test set: Accuracy: -53764472832/6950 (-773589537%)\n",
      "Test set: Accuracy: -53516361728/7000 (-764519453%)\n",
      "Test set: Accuracy: -53516361728/7050 (-759097329%)\n",
      "Test set: Accuracy: -55927705600/7100 (-787714163%)\n",
      "Test set: Accuracy: -64470470656/7150 (-901684904%)\n",
      "Test set: Accuracy: -64470470656/7200 (-895423204%)\n",
      "Test set: Accuracy: -66671161344/7250 (-919602225%)\n",
      "Test set: Accuracy: -67335835648/7300 (-922408708%)\n",
      "Test set: Accuracy: -67335835648/7350 (-916133818%)\n",
      "Test set: Accuracy: -67335835648/7400 (-909943725%)\n",
      "Test set: Accuracy: -67335835648/7450 (-903836720%)\n",
      "Test set: Accuracy: -67335835648/7500 (-897811142%)\n",
      "Test set: Accuracy: -59049644032/7550 (-782114490%)\n",
      "Test set: Accuracy: -59592290304/7600 (-784109083%)\n",
      "Test set: Accuracy: -35291332608/7650 (-461324609%)\n",
      "Test set: Accuracy: -35291332608/7700 (-458328995%)\n",
      "Test set: Accuracy: -37772546048/7750 (-487387691%)\n",
      "Test set: Accuracy: -37772546048/7800 (-484263411%)\n",
      "Test set: Accuracy: -24483192832/7850 (-311887807%)\n",
      "Test set: Accuracy: 21400139776/7900 (270887845%)\n",
      "Test set: Accuracy: 19417759744/7950 (244248550%)\n",
      "Test set: Accuracy: 19417759744/8000 (242721997%)\n",
      "Test set: Accuracy: 17724084224/8050 (220174959%)\n",
      "Test set: Accuracy: 17469900800/8100 (215677788%)\n",
      "Test set: Accuracy: 18754275328/8150 (230113808%)\n",
      "Test set: Accuracy: 18754275328/8200 (228710675%)\n",
      "Test set: Accuracy: 18838489088/8250 (228345322%)\n",
      "Test set: Accuracy: 22574149632/8300 (271977706%)\n",
      "Test set: Accuracy: 22574149632/8350 (270349097%)\n",
      "Test set: Accuracy: 22574149632/8400 (268739877%)\n",
      "Test set: Accuracy: 22574149632/8450 (267149700%)\n",
      "Test set: Accuracy: 22449932288/8500 (264116850%)\n",
      "Test set: Accuracy: 22780266496/8550 (266435865%)\n",
      "Test set: Accuracy: 22180732928/8600 (257915499%)\n",
      "Test set: Accuracy: 37995118592/8650 (439249926%)\n",
      "Test set: Accuracy: 37995118592/8700 (436725501%)\n",
      "Test set: Accuracy: 22741469184/8750 (259902505%)\n",
      "Test set: Accuracy: 24713361408/8800 (280833652%)\n",
      "Test set: Accuracy: 24713361408/8850 (279247022%)\n",
      "Test set: Accuracy: 25480536064/8900 (286298158%)\n",
      "Test set: Accuracy: 13975849984/8950 (156154748%)\n",
      "Test set: Accuracy: 13975849984/9000 (155287222%)\n",
      "Test set: Accuracy: 12475298816/9050 (137848606%)\n",
      "Test set: Accuracy: -7019192832/9100 (-77133987%)\n",
      "Test set: Accuracy: -7019192832/9150 (-76712490%)\n",
      "Test set: Accuracy: -7019192832/9200 (-76295574%)\n",
      "Test set: Accuracy: -7019192832/9250 (-75883166%)\n",
      "Test set: Accuracy: -7504723456/9300 (-80695951%)\n",
      "Test set: Accuracy: -7504723456/9350 (-80264422%)\n",
      "Test set: Accuracy: -7504723456/9400 (-79837484%)\n",
      "Test set: Accuracy: -2729225984/9450 (-28880698%)\n",
      "Test set: Accuracy: -1292192640/9500 (-13602028%)\n",
      "Test set: Accuracy: -2271035136/9550 (-23780473%)\n",
      "Test set: Accuracy: -2271034880/9600 (-23656613%)\n",
      "Test set: Accuracy: -2813680896/9650 (-29157315%)\n",
      "Test set: Accuracy: -2813680896/9700 (-29007020%)\n",
      "Test set: Accuracy: -7635490304/9750 (-78312721%)\n",
      "Test set: Accuracy: -8366139392/9800 (-85368769%)\n",
      "Test set: Accuracy: -7512425984/9850 (-76268284%)\n",
      "Test set: Accuracy: -5540004864/9900 (-55959645%)\n",
      "Test set: Accuracy: -13183398912/9950 (-132496471%)\n",
      "Test set: Accuracy: -13183398912/10000 (-131833989%)\n"
     ]
    }
   ],
   "source": [
    "test(args, model, private_test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.0 ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4c49fdcc39f9ffebd2a7e631db0f303cc4a52534a0fd828d3c1386cf51ba766a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
